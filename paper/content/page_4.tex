
\section{Challenges and Adaptations}



\subsection{Encountered Issues}

During the development of the benchmark, several challenges were encountered that required adaptations to the initial plan. The primary issues were related to the adaptation of prompt formats, the handling of completely non-functional solutions, and the execution of generated code.

\subsubsection{Generation of Unnecessary Text}
The main problem that was spoiling the results of AI models was the generation of unnecessary text. This issue was encountered in the case of gpt-3.5 and gpt-4o AI models. The models were always tring to generate additional unnecessary inroduction message, despite of all the restrictions given in the prompt. Even after improving the restrictions, GPT models were giving out the code solution with Markdown syntax, which is used for inserting code blocks in Markdown files.

\begin{lstlisting}[language=Python]{BitXorMatrix.m}
```python
def twoSum(nums, target):
    num_to_index = {}
    for index, num in enumerate(nums):
        complement = target - num
        if complement in num_to_index:
            return [num_to_index[complement], index]
        num_to_index[num] = index
```
\end{lstlisting}

\subsubsection{Wrong Code Placement}
Another issue that has been encountered was the unwillingness of the AI models to generate code that would be able to solve the problems in a proper place. Despite, writing the code, which could potentially work, AI models were completely neglecting the given template with of which they where supposed to work.

\begin{lstlisting}[language=Python]{BitXorMatrix.m}
def twoSum(nums, target):
    pass

def two_sum(nums, target):
    num_to_index = {}
    for index, num in enumerate(nums):
        complement = target - num
        if complement in num_to_index:
            return [num_to_index[complement], index]
        num_to_index[num] = index
    \end{lstlisting}

\subsection{Solutions Implemented}

Despite a seemingly difficult nature of these models' behavior, the solutions to these problems were quite simple in concept, An additional set of restrictions was set on all the prompts to make sure no extra text was added to the solution.

The issue with Markdown syntax was solved by adding a simple function that would check the first and the last line of the generated code and remove any unnecessary text.

The misplacement problem was solved by adding a comment to the template that states that the code should be written in it's place. Example:

\begin{lstlisting}[language=Python]{BitXorMatrix.m}
    def twoSum(nums, target):
        # Write your code here
        \end{lstlisting}
    

\subsection{Dropped Approaches}

During the development of the benchmark, several ideas were tested on how to execute the code solutions generated by the AI models. Adhering to the principle of using as few external modules as possible for the sake of easier codebase support in the future, the first idea was to import each solution as a module. However, the implementation of even the most basic syntax error detection proved to be unnecessarily complicated. After the decision was made to measure memory usage as well as runtime, the idea of importing each solution as a module was dropped. The main reason for this was that the memory usage of the imported module would be affected by the main process. This would make it impossible to measure the memory usage of the solution itself. That's why the final approach was to run each solution in a separate sub-process, which would allow the memory usage of the solution itself to be measured. In addition, this approach made debugging much more convenient.
















% \section{Implications and Future Directions}

% How AI models can augment current software engineering workflows (e.g., code generation, debugging, and testing).

% Potential changes to roles in the industry (e.g., humans focusing on higher-level tasks while AI handles routine coding).

% Use of AI in education or recruitment (e.g., automated assessment of programming skills).

% \subsection{Implications of Findings for Software Engineering}

% \subsubsection{Selection and Categorization of Programming Problems}

% Impact of problem diversity on AI performance.

% Lessons learned about problem selection and its role in meaningful AI evaluation.

% Recommendations for creating better problem sets in future research.

% \subsubsection{Characteristics of the Problem Set}

% How problem complexity affects AI performance.

% Importance of testing AI models on problems that mirror real-world software engineering challenges.

% The characteristics of the problem set play a crucial role in evaluating the true capabilities of AI models, as the complexity and nature of problems directly influence their performance. The problems used in this study were carefully curated to cover a wide range of scenarios that reflect both theoretical concepts and practical applications in software engineering.

% Problem complexity significantly affects AI performance. Simpler problems, often categorized as "easy," typically involve straightforward logic or basic algorithmic steps, such as iterating through an array or performing simple mathematical calculations. These problems serve as a baseline to assess the AI's ability to produce syntactically and logically correct solutions. AI models generally perform well on these tasks, as they require minimal contextual understanding and rely on commonly observed programming patterns.

% In contrast, more complex problems introduce layers of difficulty that challenge the AI's ability to reason, interpret, and optimize. Medium-difficulty problems often require intermediate skills such as recursion, efficient use of data structures like trees or heaps, or solving problems with moderate constraints. Hard problems push these boundaries further by demanding advanced algorithmic solutions, deep contextual understanding, and the ability to manage multiple interdependent variables. These challenges expose limitations in AI models, such as struggles with abstraction, lack of creativity in designing novel solutions, and inefficiency when faced with computationally intensive tasks.

% The inclusion of problems that mirror real-world software engineering scenarios is crucial for meaningful evaluation. Real-world challenges often involve incomplete or ambiguous problem statements, require multi-step reasoning, or necessitate balancing competing priorities like performance and readability. By incorporating such problems, the study ensures that the evaluation goes beyond textbook scenarios to assess the AI's practical utility. For example, problems that require handling edge cases, implementing scalable solutions, or integrating with existing systems offer valuable insights into the modelâ€™s readiness for deployment in professional environments.

% This diverse and thoughtfully constructed problem set enables a comprehensive understanding of the strengths and weaknesses of AI models. It highlights not only their technical capabilities but also their potential limitations when faced with real-world complexity, ultimately guiding future research and development in AI-driven programming tools.
% \subsection{Challenges and Limitations}

% Limitations of the study (e.g., focus on Leetcode, which may not reflect real-world scenarios).

% Challenges in evaluating non-deterministic AI outputs.

% Biases in the dataset or prompts that may influence AI performance.

% \subsection{Future Potential}

% The evolution of AI in software engineering presents a range of opportunities for advancing its role beyond current capabilities. As these models grow more sophisticated, they may achieve a deeper understanding of problem contexts, allowing them to address challenges involving ambiguity or incomplete information with greater precision. By refining their ability to interpret complex requirements, AI systems could become indispensable for tackling tasks that demand both technical knowledge and contextual reasoning.

% In addition to independent problem-solving, AI has the potential to become a more effective collaborator in team-based programming environments. By integrating seamlessly into workflows, these models could assist developers in real time, suggesting optimizations, identifying errors, and improving overall code quality. Such advancements would position AI as a trusted partner in the development process, augmenting rather than replacing human expertise.

% Another promising direction lies in enabling AI to handle large-scale, real-world projects. While current models are adept at solving isolated problems, future systems could extend their functionality to manage multi-file software projects with complex dependencies. This would open doors to applications in fields such as enterprise software development, where AI could assist in creating, maintaining, and scaling systems efficiently.

% The expansion of benchmarks and evaluation frameworks will also play a critical role in shaping the trajectory of AI in programming. By introducing benchmarks that better reflect real-world challenges, such as collaborative tasks or system design problems, researchers can encourage the development of models that align closely with industry needs. This will help bridge the gap between academic research and practical applications, ensuring AI evolves in ways that are directly beneficial to software engineering.

% The ethical implications of relying on AI for critical systems also demand attention. As these systems become more capable, it will be essential to address concerns such as accountability, transparency, and the potential displacement of human roles. By prioritizing responsible development and deployment, AI can be integrated into software engineering in a way that maximizes its benefits while minimizing risks.

% Future advancements in AI for software engineering are likely to reshape the profession in profound ways, enhancing productivity, enabling new possibilities, and driving innovation across the field. As these systems continue to evolve, their role will become increasingly central to the way software is designed, developed, and maintained.








% Suggestions for improving AI models (e.g., better training on real-world coding patterns, handling ambiguities).

% Expanding evaluations to include team-based programming or larger-scale projects.

% Creation of additional benchmarks to assess AI performance in different software engineering tasks (Use my old ideas).



% Do not add the code in Appendix, it is not necessary.