
\section{Challenges and Adaptations}



\subsection{Encountered Issues}

During the development process, a number of challenges were identified that necessitated adjustments to the initial plan. The primary issues were related to the adaptation of prompt formats, the handling of completely non-functional solutions, and the execution of generated code.
\subsubsection{Generation of Unnecessary Text}
The primary issue that compromised the performance of AI models was the generation of superfluous text. This problem was observed in the GPT-3.5 and GPT-4 models. Despite the imposed limitations in the prompt, the models persistently attempted to generate additional, unnecessary introductory messages. Despite the implementation of enhanced restrictions, GPT models continued to produce code solutions using Markdown syntax, which is employed for inserting code blocks in Markdown files.

\begin{lstlisting}[language=Python]{BitXorMatrix.m}
```python
def twoSum(nums, target):
    num_to_index = {}
    for index, num in enumerate(nums):
        complement = target - num
        if complement in num_to_index:
            return [num_to_index[complement], index]
        num_to_index[num] = index
```
\end{lstlisting}

\subsubsection{Wrong Code Placement}
A further issue that has been identified is the inability of the AI models to generate code that can effectively solve problems in the correct location. Despite the creation of code that could potentially be effective, the AI models disregarded the provided template, for which they were designed to be used.

\begin{lstlisting}[language=Python]{BitXorMatrix.m}
def twoSum(nums, target):
    pass

def two_sum(nums, target):
    num_to_index = {}
    for index, num in enumerate(nums):
        complement = target - num
        if complement in num_to_index:
            return [num_to_index[complement], index]
        num_to_index[num] = index
    \end{lstlisting}

\subsection{Solutions Implemented}

Despite the complexity of these models' behaviour, the solutions to these problems were quite simple in concept.An additional set of restrictions was applied to all the prompts to ensure that no extra text was added to the solution.

The issue with Markdown syntax was resolved by implementing a straightforward function that would verify the first and last lines of the generated code and eliminate any superfluous text.

The misplacement issue was addressed by adding a comment to the template instructing that the code should be written in its place. For illustrative purposes, please refer to the following example:
\begin{lstlisting}[language=Python]{BitXorMatrix.m}
    def twoSum(nums, target):
        # Write your code here
        \end{lstlisting}
    

\subsection{Dropped Approaches}

During the development of the benchmark, several ideas were tested on how to execute the code solutions generated by the AI models. In accordance with the principle of using as few external modules as possible to facilitate easier codebase support in the future, the initial concept was to import each solution as a module. However, the implementation of even the most basic syntax error detection proved to be unnecessarily complicated. Following the decision to measure memory usage as well as runtime, the idea of importing each solution as a module was abandoned. The primary reason for this was that the memory usage of the imported module would be affected by the main process. This would hinder the ability to accurately measure the memory usage of the solution itself. The final approach adopted was to run each solution in a separate sub-process, enabling the memory usage of the solution itself to be measured. This approach also greatly facilitates debugging.















% \section{Implications and Future Directions}

% How AI models can augment current software engineering workflows (e.g., code generation, debugging, and testing).

% Potential changes to roles in the industry (e.g., humans focusing on higher-level tasks while AI handles routine coding).

% Use of AI in education or recruitment (e.g., automated assessment of programming skills).

% \subsection{Implications of Findings for Software Engineering}

% \subsubsection{Selection and Categorization of Programming Problems}

% Impact of problem diversity on AI performance.

% Lessons learned about problem selection and its role in meaningful AI evaluation.

% Recommendations for creating better problem sets in future research.

% \subsubsection{Characteristics of the Problem Set}

% How problem complexity affects AI performance.

% Importance of testing AI models on problems that mirror real-world software engineering challenges.

% The characteristics of the problem set play a crucial role in evaluating the true capabilities of AI models, as the complexity and nature of problems directly influence their performance. The problems used in this study were carefully curated to cover a wide range of scenarios that reflect both theoretical concepts and practical applications in software engineering.

% Problem complexity significantly affects AI performance. Simpler problems, often categorized as "easy," typically involve straightforward logic or basic algorithmic steps, such as iterating through an array or performing simple mathematical calculations. These problems serve as a baseline to assess the AI's ability to produce syntactically and logically correct solutions. AI models generally perform well on these tasks, as they require minimal contextual understanding and rely on commonly observed programming patterns.

% In contrast, more complex problems introduce layers of difficulty that challenge the AI's ability to reason, interpret, and optimize. Medium-difficulty problems often require intermediate skills such as recursion, efficient use of data structures like trees or heaps, or solving problems with moderate constraints. Hard problems push these boundaries further by demanding advanced algorithmic solutions, deep contextual understanding, and the ability to manage multiple interdependent variables. These challenges expose limitations in AI models, such as struggles with abstraction, lack of creativity in designing novel solutions, and inefficiency when faced with computationally intensive tasks.

% The inclusion of problems that mirror real-world software engineering scenarios is crucial for meaningful evaluation. Real-world challenges often involve incomplete or ambiguous problem statements, require multi-step reasoning, or necessitate balancing competing priorities like performance and readability. By incorporating such problems, the study ensures that the evaluation goes beyond textbook scenarios to assess the AI's practical utility. For example, problems that require handling edge cases, implementing scalable solutions, or integrating with existing systems offer valuable insights into the modelâ€™s readiness for deployment in professional environments.

% This diverse and thoughtfully constructed problem set enables a comprehensive understanding of the strengths and weaknesses of AI models. It highlights not only their technical capabilities but also their potential limitations when faced with real-world complexity, ultimately guiding future research and development in AI-driven programming tools.
% \subsection{Challenges and Limitations}

% Limitations of the study (e.g., focus on Leetcode, which may not reflect real-world scenarios).

% Challenges in evaluating non-deterministic AI outputs.

% Biases in the dataset or prompts that may influence AI performance.

% \subsection{Future Potential}

% The evolution of AI in software engineering presents a range of opportunities for advancing its role beyond current capabilities. As these models grow more sophisticated, they may achieve a deeper understanding of problem contexts, allowing them to address challenges involving ambiguity or incomplete information with greater precision. By refining their ability to interpret complex requirements, AI systems could become indispensable for tackling tasks that demand both technical knowledge and contextual reasoning.

% In addition to independent problem-solving, AI has the potential to become a more effective collaborator in team-based programming environments. By integrating seamlessly into workflows, these models could assist developers in real time, suggesting optimizations, identifying errors, and improving overall code quality. Such advancements would position AI as a trusted partner in the development process, augmenting rather than replacing human expertise.

% Another promising direction lies in enabling AI to handle large-scale, real-world projects. While current models are adept at solving isolated problems, future systems could extend their functionality to manage multi-file software projects with complex dependencies. This would open doors to applications in fields such as enterprise software development, where AI could assist in creating, maintaining, and scaling systems efficiently.

% The expansion of benchmarks and evaluation frameworks will also play a critical role in shaping the trajectory of AI in programming. By introducing benchmarks that better reflect real-world challenges, such as collaborative tasks or system design problems, researchers can encourage the development of models that align closely with industry needs. This will help bridge the gap between academic research and practical applications, ensuring AI evolves in ways that are directly beneficial to software engineering.

% The ethical implications of relying on AI for critical systems also demand attention. As these systems become more capable, it will be essential to address concerns such as accountability, transparency, and the potential displacement of human roles. By prioritizing responsible development and deployment, AI can be integrated into software engineering in a way that maximizes its benefits while minimizing risks.

% Future advancements in AI for software engineering are likely to reshape the profession in profound ways, enhancing productivity, enabling new possibilities, and driving innovation across the field. As these systems continue to evolve, their role will become increasingly central to the way software is designed, developed, and maintained.








% Suggestions for improving AI models (e.g., better training on real-world coding patterns, handling ambiguities).

% Expanding evaluations to include team-based programming or larger-scale projects.

% Creation of additional benchmarks to assess AI performance in different software engineering tasks (Use my old ideas).



% Do not add the code in Appendix, it is not necessary.