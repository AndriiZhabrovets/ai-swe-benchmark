
\section*{Abstract}

The rapid advancement of artificial intelligence (AI) has brought significant attention to its potential in software engineering, particularly in automating programming tasks. This study evaluates the capabilities of state-of-the-art AI models, including GPT-4, GPT-3.5, Claude, and Gemini, in solving algorithmic challenges. Using a benchmark of curated problems from LeetCode, the research assesses each model's success rate, runtime, and memory usage across varying levels of difficulty. Results highlight distinct strengths and limitations among the models, with GPT-4 demonstrating superior accuracy but higher resource consumption compared to others. The study identifies persistent challenges, such as handling edge cases and ensuring efficiency, while also exploring practical applications and adaptations for AI in real-world programming. By establishing a robust framework for evaluation, this research contributes to understanding AI's evolving role in software development and lays the groundwork for future advancements in AI-driven engineering solutions.

